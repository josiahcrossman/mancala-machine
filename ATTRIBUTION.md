# Attribution:

## AI-Generated Code:

I used cursor throughout the development process, and frequently hit tab when writing code. Blocks of code where this happened a significant amount, such as the function self_play() or the class MancalaEnv, are marked with a comment at the top

The update() function in ReinforceAgent was mostly AI generated, as while I understood the mathematical formula and theory for the REINFORCE algorithm, I used Cursor to help me implement it in code. 

Action_select() in MancalaAgent and ReinforceAgent was revised from my initial code with the help of Cursor to be more effecient and work properly. 

I had Cursor generate a small amount of code at the bottom of self_play() and reinforce_self_play() to convert an array of win percentages into a graph

The compute_returns() function in Reinforce_agent() was autogenerated by Cursor tab. 


## Other Sources:

<!-- Cite the confusion matrix source, as well as other REINFORCE sources. Also check Cursor history to see if I missed anything-->

I reused some of the agent architecture/training loop architecture from homework 8, specifically the NeuralNetwork class, a lot of the logic for the MancalaAgent class for deep-q learning, and the training loop of updating the deep-q weights. I then reused some of the MancalaAgent and training loop logic for the policy-based learning and play_random/agent_vs_agent functionality, but to a lesser extent. 

I referenced [this](https://huggingface.co/learn/deep-rl-course/en/unit7/self-play) huggingface tutorial about self-play when creating the self-play trianing logic, and reused some of the ideas they came up with, especially the idea of having a queue of agents that's randomly chosen, as well as occasionally playing against the current agent. No code was reused, but some theory was. 

I used the gymansium [documentation](https://gymnasium.farama.org/introduction/create_custom_env/#step-function) when figuring out how to set up the custom Mancala environment. I copied their code for registering the environment, and referenced the rest for the reset(), step() and init() functions

For implementing the REINFORCE algorithm in code in the ReinforceAgent class, I consulted [geeksforgeeks](https://www.geeksforgeeks.org/machine-learning/reinforce-algorithm/) and a tutorial on [medium](https://medium.com/@sofeikov/reinforce-algorithm-reinforcement-learning-from-scratch-in-pytorch-41fcccafa107). I didn't end up directly using code from either source, but they were references in my understanding. 

Used code from the sklearn [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay) to plot the ConfusionMatrixDisplay in notebooks/evaluation.ipynb 

Website used for video demo: https://www.crazygames.com/game/mancala-classic
